% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/context_chat_client.R
\name{context_chat_client}
\alias{context_chat_client}
\title{context_chat_client  with auto-shrink + optional model}
\usage{
context_chat_client(
  model = getOption("contextR.chat_model", NULL),
  k = 10,
  system_prompt = "Answer concisely and use prior context.",
  summary_n = NULL,
  summary_system = getOption("contextR.summary_system",
    "You are a concise scribe. Summarize faithfully in <=120 words; include decisions, actions, open questions."),
  ...
)
}
\arguments{
\item{model}{Bedrock model ID.  If \code{NULL}, the client will look for
\code{getOption("contextR.chat_model")}; if that too is \code{NULL},
it lets \code{ellmer::chat_aws_bedrock()} choose its default.}

\item{k}{Maximum total rows kept in the buffer (raw + summaries).}

\item{system_prompt}{System prompt shown to the LLM at client creation.}

\item{summary_n}{Summarise every \emph{n} raw turns (user + assistant).
Use \code{NULL} to disable summaries.}

\item{summary_system}{System prompt for the summariser.}

\item{...}{Extra args passed to \code{ellmer::chat_aws_bedrock()}.}
}
\description{
Create a persistent chat client that maintains conversational memory
(with optional rolling summaries) and talks to Bedrock via \strong{ellmer}.
}
